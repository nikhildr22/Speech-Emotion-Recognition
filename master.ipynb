{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "master.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxZSSC5W8Vdiy1/ZQ8aXEq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhildr22/Speech-Emotion-Recognition/blob/master/master.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDxKDAeHq3Eo",
        "colab_type": "code",
        "outputId": "0808708f-2ede-4fb4-ca59-ab78990a8edd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2OyhiRinXTI",
        "colab_type": "code",
        "outputId": "dcb60766-0cc5-46d6-dfa4-a8e0d8418275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxS4uqaWtesi",
        "colab_type": "code",
        "outputId": "c220dc3a-fb70-4a3e-9266-6f1c5c5d2afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "!wget https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-16 15:40:25--  https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.184.117.155\n",
            "Connecting to zenodo.org (zenodo.org)|188.184.117.155|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 208468073 (199M) [application/octet-stream]\n",
            "Saving to: ‘Audio_Speech_Actors_01-24.zip’\n",
            "\n",
            "Audio_Speech_Actors 100%[===================>] 198.81M  80.5MB/s    in 2.5s    \n",
            "\n",
            "2020-04-16 15:40:28 (80.5 MB/s) - ‘Audio_Speech_Actors_01-24.zip’ saved [208468073/208468073]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGyGSDSGv0x2",
        "colab_type": "code",
        "outputId": "293a5b02-4d29-457a-ba7f-8dedd1cbd9a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!mkdir dataset\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Audio_Speech_Actors_01-24.zip  dataset\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh9M1cffv9CZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q Audio_Speech_Actors_01-24.zip -d dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKzEkfDGwhLE",
        "colab_type": "code",
        "outputId": "51593e00-cd1c-4337-f3a1-528d332a9f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Audio_Speech_Actors_01-24.zip  dataset\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AErppsmXyxyD",
        "colab_type": "code",
        "outputId": "8781aa49-310e-4d4a-bbc2-f43eca404164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "!pip install soundfile"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Installing collected packages: soundfile\n",
            "Successfully installed soundfile-0.10.3.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzhsncx9zO35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "from tqdm import tqdm\n",
        "import soundfile\n",
        "from scipy.io import wavfile\n",
        "import os, glob, pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX8xLyw9GPpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' directory to keep clean files '''\n",
        "!mkdir clean "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaoEOadGzTwb",
        "colab_type": "code",
        "outputId": "c20732f9-7bd5-49a9-8917-94638a6f9274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "''' downsampling audio files to 16KHz and storing them in /clean directory ''' \n",
        "for f in tqdm(glob.glob(\"dataset/*/*\")):\n",
        "    signal, rate = librosa.load(path=f, sr=16000)\n",
        "    wavfile.write(filename='clean/'+f[-24:],rate=rate,data=signal)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1440/1440 [05:49<00:00,  4.12it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaDIo9MkzbxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract features (mfcc, chroma, mel) from a sound file\n",
        "def extract_feature(file_name, mfcc, chroma, mel):\n",
        "    with soundfile.SoundFile(file_name) as sound_file:\n",
        "        X = sound_file.read(dtype=\"float32\")\n",
        "        sample_rate=sound_file.samplerate\n",
        "        if chroma:\n",
        "            stft=np.abs(librosa.stft(X))\n",
        "        result=np.array([])\n",
        "        if mfcc:\n",
        "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "            result=np.hstack((result, mfccs))\n",
        "        if chroma:\n",
        "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
        "            result=np.hstack((result, chroma))\n",
        "        if mel:\n",
        "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
        "            result=np.hstack((result, mel))\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfOX_3jjzflf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Emotions in the RAVDESS dataset\n",
        "emotions={\n",
        "  '01':'neutral',\n",
        "  '02':'calm',\n",
        "  '03':'happy',\n",
        "  '04':'sad',\n",
        "  '05':'angry',\n",
        "  '06':'fearful',\n",
        "  '07':'disgust',\n",
        "  '08':'surprised'\n",
        "}\n",
        "# Emotions to observe (remove the prefix \"/\" to include that emotion)\n",
        "observed_emotions=['sad','angry','happy','neutral','/calm','/fearful','/disgust','/surprised']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUx8FVe6zjdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data and extract features for each sound file\n",
        "\n",
        "def load_data(test_size=0.2):\n",
        "    x,y=[],[] \n",
        "    for file in tqdm(glob.glob(\"clean/*\")):\n",
        "        file_name=os.path.basename(file[-24:])\n",
        "#         print(file_name)\n",
        "        emotion=emotions[file_name.split(\"-\")[2]]\n",
        "        if emotion not in observed_emotions:\n",
        "            continue\n",
        "        try:\n",
        "            feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
        "            x.append(feature)\n",
        "            y.append(int(file_name.split(\"-\")[2]))\n",
        "        except:continue\n",
        "    return train_test_split(np.array(x), np.array(y), test_size=test_size, random_state=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkdO_qK4znlH",
        "colab_type": "code",
        "outputId": "5a8f3197-31f7-4696-c8ab-87359a73deed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Split the dataset\n",
        "x_train,x_test,y_train,y_test=load_data(test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1440/1440 [00:33<00:00, 42.74it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV5hZlNDzwvP",
        "colab_type": "code",
        "outputId": "24136829-24c9-4a95-f289-efb906212bc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Get the shape of the training and testing datasets\n",
        "print((x_train.shape[0], x_test.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(470, 202)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCWcNL950Xrl",
        "colab_type": "code",
        "outputId": "cdba0a4e-d171-415a-f8c0-f6c93f4e5ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Get the number of features extracted\n",
        "print(f'Features extracted: {x_train.shape[1]}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features extracted: 180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrcyHTbU0aDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize the Multi Layer Perceptron Classifier\n",
        "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500,verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnEx--Er0dc8",
        "colab_type": "code",
        "outputId": "746dc9dc-575b-440a-ca66-3f6016eed73b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#DataFlair - Train the model\n",
        "model.fit(x_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 22.87191766\n",
            "Iteration 2, loss = 11.22955960\n",
            "Iteration 3, loss = 4.57241991\n",
            "Iteration 4, loss = 8.94152890\n",
            "Iteration 5, loss = 7.09372804\n",
            "Iteration 6, loss = 5.64829204\n",
            "Iteration 7, loss = 2.38699633\n",
            "Iteration 8, loss = 3.73084042\n",
            "Iteration 9, loss = 3.69471201\n",
            "Iteration 10, loss = 3.52600292\n",
            "Iteration 11, loss = 3.03349813\n",
            "Iteration 12, loss = 1.85571044\n",
            "Iteration 13, loss = 2.22516522\n",
            "Iteration 14, loss = 2.21240703\n",
            "Iteration 15, loss = 1.89134670\n",
            "Iteration 16, loss = 1.47478099\n",
            "Iteration 17, loss = 1.71333958\n",
            "Iteration 18, loss = 1.18655212\n",
            "Iteration 19, loss = 1.44503522\n",
            "Iteration 20, loss = 1.29782545\n",
            "Iteration 21, loss = 1.09038538\n",
            "Iteration 22, loss = 1.22090763\n",
            "Iteration 23, loss = 1.07245173\n",
            "Iteration 24, loss = 1.06101409\n",
            "Iteration 25, loss = 1.02308693\n",
            "Iteration 26, loss = 0.96864344\n",
            "Iteration 27, loss = 0.95421722\n",
            "Iteration 28, loss = 0.89689903\n",
            "Iteration 29, loss = 0.90674557\n",
            "Iteration 30, loss = 0.86290837\n",
            "Iteration 31, loss = 0.86180472\n",
            "Iteration 32, loss = 0.82895482\n",
            "Iteration 33, loss = 0.82563687\n",
            "Iteration 34, loss = 0.81718700\n",
            "Iteration 35, loss = 0.81007368\n",
            "Iteration 36, loss = 0.78116910\n",
            "Iteration 37, loss = 0.78736945\n",
            "Iteration 38, loss = 0.76617171\n",
            "Iteration 39, loss = 0.75903843\n",
            "Iteration 40, loss = 0.75344799\n",
            "Iteration 41, loss = 0.75432370\n",
            "Iteration 42, loss = 0.73627238\n",
            "Iteration 43, loss = 0.74899083\n",
            "Iteration 44, loss = 0.73239867\n",
            "Iteration 45, loss = 0.72506342\n",
            "Iteration 46, loss = 0.72162002\n",
            "Iteration 47, loss = 0.70539749\n",
            "Iteration 48, loss = 0.71595523\n",
            "Iteration 49, loss = 0.72235365\n",
            "Iteration 50, loss = 0.69407555\n",
            "Iteration 51, loss = 0.70125829\n",
            "Iteration 52, loss = 0.70149178\n",
            "Iteration 53, loss = 0.69317389\n",
            "Iteration 54, loss = 0.69821140\n",
            "Iteration 55, loss = 0.69021393\n",
            "Iteration 56, loss = 0.67232232\n",
            "Iteration 57, loss = 0.68218349\n",
            "Iteration 58, loss = 0.67466720\n",
            "Iteration 59, loss = 0.67558033\n",
            "Iteration 60, loss = 0.67206349\n",
            "Iteration 61, loss = 0.66975165\n",
            "Iteration 62, loss = 0.65703707\n",
            "Iteration 63, loss = 0.66848526\n",
            "Iteration 64, loss = 0.66233579\n",
            "Iteration 65, loss = 0.65785662\n",
            "Iteration 66, loss = 0.64506521\n",
            "Iteration 67, loss = 0.64428552\n",
            "Iteration 68, loss = 0.64173729\n",
            "Iteration 69, loss = 0.64948973\n",
            "Iteration 70, loss = 0.64000857\n",
            "Iteration 71, loss = 0.63524248\n",
            "Iteration 72, loss = 0.63247783\n",
            "Iteration 73, loss = 0.61200494\n",
            "Iteration 74, loss = 0.62285960\n",
            "Iteration 75, loss = 0.62778048\n",
            "Iteration 76, loss = 0.62063870\n",
            "Iteration 77, loss = 0.63083462\n",
            "Iteration 78, loss = 0.60143787\n",
            "Iteration 79, loss = 0.60258348\n",
            "Iteration 80, loss = 0.61344681\n",
            "Iteration 81, loss = 0.59017810\n",
            "Iteration 82, loss = 0.58721889\n",
            "Iteration 83, loss = 0.58221047\n",
            "Iteration 84, loss = 0.58918876\n",
            "Iteration 85, loss = 0.57934006\n",
            "Iteration 86, loss = 0.57034191\n",
            "Iteration 87, loss = 0.56006829\n",
            "Iteration 88, loss = 0.57652078\n",
            "Iteration 89, loss = 0.56058864\n",
            "Iteration 90, loss = 0.55768390\n",
            "Iteration 91, loss = 0.56283998\n",
            "Iteration 92, loss = 0.54257281\n",
            "Iteration 93, loss = 0.54814101\n",
            "Iteration 94, loss = 0.56513857\n",
            "Iteration 95, loss = 0.54208300\n",
            "Iteration 96, loss = 0.55839923\n",
            "Iteration 97, loss = 0.53394485\n",
            "Iteration 98, loss = 0.54338795\n",
            "Iteration 99, loss = 0.54506741\n",
            "Iteration 100, loss = 0.55335806\n",
            "Iteration 101, loss = 0.53120569\n",
            "Iteration 102, loss = 0.52998574\n",
            "Iteration 103, loss = 0.54058050\n",
            "Iteration 104, loss = 0.50402268\n",
            "Iteration 105, loss = 0.50799915\n",
            "Iteration 106, loss = 0.51010029\n",
            "Iteration 107, loss = 0.49436753\n",
            "Iteration 108, loss = 0.50081296\n",
            "Iteration 109, loss = 0.50409450\n",
            "Iteration 110, loss = 0.49188469\n",
            "Iteration 111, loss = 0.49925903\n",
            "Iteration 112, loss = 0.49494869\n",
            "Iteration 113, loss = 0.51415733\n",
            "Iteration 114, loss = 0.51079670\n",
            "Iteration 115, loss = 0.49096363\n",
            "Iteration 116, loss = 0.49710002\n",
            "Iteration 117, loss = 0.46761218\n",
            "Iteration 118, loss = 0.48324872\n",
            "Iteration 119, loss = 0.46006984\n",
            "Iteration 120, loss = 0.49548592\n",
            "Iteration 121, loss = 0.49375367\n",
            "Iteration 122, loss = 0.47197657\n",
            "Iteration 123, loss = 0.49806544\n",
            "Iteration 124, loss = 0.45285726\n",
            "Iteration 125, loss = 0.46881632\n",
            "Iteration 126, loss = 0.46855591\n",
            "Iteration 127, loss = 0.43964801\n",
            "Iteration 128, loss = 0.44148252\n",
            "Iteration 129, loss = 0.44035169\n",
            "Iteration 130, loss = 0.44523058\n",
            "Iteration 131, loss = 0.47313388\n",
            "Iteration 132, loss = 0.46650918\n",
            "Iteration 133, loss = 0.47515114\n",
            "Iteration 134, loss = 0.44940147\n",
            "Iteration 135, loss = 0.44560040\n",
            "Iteration 136, loss = 0.45696086\n",
            "Iteration 137, loss = 0.43179724\n",
            "Iteration 138, loss = 0.44357338\n",
            "Iteration 139, loss = 0.40962024\n",
            "Iteration 140, loss = 0.40710765\n",
            "Iteration 141, loss = 0.41161774\n",
            "Iteration 142, loss = 0.39635170\n",
            "Iteration 143, loss = 0.38701639\n",
            "Iteration 144, loss = 0.41982413\n",
            "Iteration 145, loss = 0.39065408\n",
            "Iteration 146, loss = 0.38540219\n",
            "Iteration 147, loss = 0.37999791\n",
            "Iteration 148, loss = 0.37857890\n",
            "Iteration 149, loss = 0.37037190\n",
            "Iteration 150, loss = 0.36063769\n",
            "Iteration 151, loss = 0.36576978\n",
            "Iteration 152, loss = 0.36230929\n",
            "Iteration 153, loss = 0.35107079\n",
            "Iteration 154, loss = 0.36819431\n",
            "Iteration 155, loss = 0.35703656\n",
            "Iteration 156, loss = 0.35400867\n",
            "Iteration 157, loss = 0.34899748\n",
            "Iteration 158, loss = 0.35158447\n",
            "Iteration 159, loss = 0.34208279\n",
            "Iteration 160, loss = 0.34430148\n",
            "Iteration 161, loss = 0.35375705\n",
            "Iteration 162, loss = 0.34663602\n",
            "Iteration 163, loss = 0.34687543\n",
            "Iteration 164, loss = 0.35707071\n",
            "Iteration 165, loss = 0.34817146\n",
            "Iteration 166, loss = 0.33692487\n",
            "Iteration 167, loss = 0.32550630\n",
            "Iteration 168, loss = 0.32514070\n",
            "Iteration 169, loss = 0.34147654\n",
            "Iteration 170, loss = 0.33892450\n",
            "Iteration 171, loss = 0.33642210\n",
            "Iteration 172, loss = 0.31999405\n",
            "Iteration 173, loss = 0.32074363\n",
            "Iteration 174, loss = 0.32301875\n",
            "Iteration 175, loss = 0.30641022\n",
            "Iteration 176, loss = 0.30413920\n",
            "Iteration 177, loss = 0.32364096\n",
            "Iteration 178, loss = 0.30390081\n",
            "Iteration 179, loss = 0.30605435\n",
            "Iteration 180, loss = 0.29830193\n",
            "Iteration 181, loss = 0.30309629\n",
            "Iteration 182, loss = 0.30588161\n",
            "Iteration 183, loss = 0.32185253\n",
            "Iteration 184, loss = 0.30974395\n",
            "Iteration 185, loss = 0.30284624\n",
            "Iteration 186, loss = 0.30475465\n",
            "Iteration 187, loss = 0.29686695\n",
            "Iteration 188, loss = 0.29384916\n",
            "Iteration 189, loss = 0.29169369\n",
            "Iteration 190, loss = 0.28773524\n",
            "Iteration 191, loss = 0.28957809\n",
            "Iteration 192, loss = 0.29240863\n",
            "Iteration 193, loss = 0.29586124\n",
            "Iteration 194, loss = 0.27093949\n",
            "Iteration 195, loss = 0.27891659\n",
            "Iteration 196, loss = 0.28325862\n",
            "Iteration 197, loss = 0.27661279\n",
            "Iteration 198, loss = 0.27282563\n",
            "Iteration 199, loss = 0.27632342\n",
            "Iteration 200, loss = 0.27764995\n",
            "Iteration 201, loss = 0.27028555\n",
            "Iteration 202, loss = 0.27571258\n",
            "Iteration 203, loss = 0.29715786\n",
            "Iteration 204, loss = 0.29339062\n",
            "Iteration 205, loss = 0.27835389\n",
            "Iteration 206, loss = 0.27028740\n",
            "Iteration 207, loss = 0.27153521\n",
            "Iteration 208, loss = 0.25618364\n",
            "Iteration 209, loss = 0.26571119\n",
            "Iteration 210, loss = 0.24707183\n",
            "Iteration 211, loss = 0.26833963\n",
            "Iteration 212, loss = 0.26048076\n",
            "Iteration 213, loss = 0.25134914\n",
            "Iteration 214, loss = 0.24294996\n",
            "Iteration 215, loss = 0.24461356\n",
            "Iteration 216, loss = 0.24580088\n",
            "Iteration 217, loss = 0.24801307\n",
            "Iteration 218, loss = 0.23483582\n",
            "Iteration 219, loss = 0.23968672\n",
            "Iteration 220, loss = 0.23522177\n",
            "Iteration 221, loss = 0.22917319\n",
            "Iteration 222, loss = 0.22991921\n",
            "Iteration 223, loss = 0.22034549\n",
            "Iteration 224, loss = 0.22595544\n",
            "Iteration 225, loss = 0.22115760\n",
            "Iteration 226, loss = 0.22896451\n",
            "Iteration 227, loss = 0.22440594\n",
            "Iteration 228, loss = 0.22599423\n",
            "Iteration 229, loss = 0.21757703\n",
            "Iteration 230, loss = 0.21385187\n",
            "Iteration 231, loss = 0.21849447\n",
            "Iteration 232, loss = 0.22649873\n",
            "Iteration 233, loss = 0.21814276\n",
            "Iteration 234, loss = 0.21764580\n",
            "Iteration 235, loss = 0.22382373\n",
            "Iteration 236, loss = 0.20179209\n",
            "Iteration 237, loss = 0.22095852\n",
            "Iteration 238, loss = 0.21548312\n",
            "Iteration 239, loss = 0.21104660\n",
            "Iteration 240, loss = 0.20548733\n",
            "Iteration 241, loss = 0.21159178\n",
            "Iteration 242, loss = 0.20749873\n",
            "Iteration 243, loss = 0.21003466\n",
            "Iteration 244, loss = 0.20724812\n",
            "Iteration 245, loss = 0.20848341\n",
            "Iteration 246, loss = 0.21948673\n",
            "Iteration 247, loss = 0.19851906\n",
            "Iteration 248, loss = 0.20341622\n",
            "Iteration 249, loss = 0.20178889\n",
            "Iteration 250, loss = 0.21406238\n",
            "Iteration 251, loss = 0.19428423\n",
            "Iteration 252, loss = 0.19765858\n",
            "Iteration 253, loss = 0.21739523\n",
            "Iteration 254, loss = 0.21122481\n",
            "Iteration 255, loss = 0.20575078\n",
            "Iteration 256, loss = 0.19911010\n",
            "Iteration 257, loss = 0.18932936\n",
            "Iteration 258, loss = 0.19331683\n",
            "Iteration 259, loss = 0.19634327\n",
            "Iteration 260, loss = 0.19153379\n",
            "Iteration 261, loss = 0.18913193\n",
            "Iteration 262, loss = 0.17715401\n",
            "Iteration 263, loss = 0.18158168\n",
            "Iteration 264, loss = 0.17654786\n",
            "Iteration 265, loss = 0.18211895\n",
            "Iteration 266, loss = 0.17349480\n",
            "Iteration 267, loss = 0.17121700\n",
            "Iteration 268, loss = 0.17151651\n",
            "Iteration 269, loss = 0.16847660\n",
            "Iteration 270, loss = 0.16964279\n",
            "Iteration 271, loss = 0.16988050\n",
            "Iteration 272, loss = 0.17628622\n",
            "Iteration 273, loss = 0.17390595\n",
            "Iteration 274, loss = 0.16988717\n",
            "Iteration 275, loss = 0.17181068\n",
            "Iteration 276, loss = 0.16829101\n",
            "Iteration 277, loss = 0.16571696\n",
            "Iteration 278, loss = 0.16643328\n",
            "Iteration 279, loss = 0.17433099\n",
            "Iteration 280, loss = 0.16277125\n",
            "Iteration 281, loss = 0.17001378\n",
            "Iteration 282, loss = 0.16624126\n",
            "Iteration 283, loss = 0.17150786\n",
            "Iteration 284, loss = 0.15950822\n",
            "Iteration 285, loss = 0.15675208\n",
            "Iteration 286, loss = 0.15759545\n",
            "Iteration 287, loss = 0.16476885\n",
            "Iteration 288, loss = 0.15926829\n",
            "Iteration 289, loss = 0.16126912\n",
            "Iteration 290, loss = 0.15923436\n",
            "Iteration 291, loss = 0.15814736\n",
            "Iteration 292, loss = 0.15471269\n",
            "Iteration 293, loss = 0.15845403\n",
            "Iteration 294, loss = 0.15313753\n",
            "Iteration 295, loss = 0.15452757\n",
            "Iteration 296, loss = 0.15513481\n",
            "Iteration 297, loss = 0.14904753\n",
            "Iteration 298, loss = 0.14571202\n",
            "Iteration 299, loss = 0.14380505\n",
            "Iteration 300, loss = 0.14299581\n",
            "Iteration 301, loss = 0.14503489\n",
            "Iteration 302, loss = 0.14271380\n",
            "Iteration 303, loss = 0.14661579\n",
            "Iteration 304, loss = 0.14080305\n",
            "Iteration 305, loss = 0.14179838\n",
            "Iteration 306, loss = 0.14076398\n",
            "Iteration 307, loss = 0.14037673\n",
            "Iteration 308, loss = 0.14048058\n",
            "Iteration 309, loss = 0.13650087\n",
            "Iteration 310, loss = 0.13589355\n",
            "Iteration 311, loss = 0.13798255\n",
            "Iteration 312, loss = 0.13870132\n",
            "Iteration 313, loss = 0.13757090\n",
            "Iteration 314, loss = 0.15096132\n",
            "Iteration 315, loss = 0.13937898\n",
            "Iteration 316, loss = 0.14060073\n",
            "Iteration 317, loss = 0.13281739\n",
            "Iteration 318, loss = 0.13709870\n",
            "Iteration 319, loss = 0.13439033\n",
            "Iteration 320, loss = 0.13463189\n",
            "Iteration 321, loss = 0.14369265\n",
            "Iteration 322, loss = 0.14893093\n",
            "Iteration 323, loss = 0.16117989\n",
            "Iteration 324, loss = 0.14893116\n",
            "Iteration 325, loss = 0.14950102\n",
            "Iteration 326, loss = 0.13546190\n",
            "Iteration 327, loss = 0.14351912\n",
            "Iteration 328, loss = 0.14950007\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.01, batch_size=256, beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(300,), learning_rate='adaptive',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7wLIZ6a0gTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predict for the test set\n",
        "y_pred=model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgyDehEE0vx3",
        "colab_type": "code",
        "outputId": "5b593407-843f-4795-b554-647cac85d920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Calculate the accuracy of our model\n",
        "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
        "#Print the accuracy\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 73.27%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woCm2Cps0yKh",
        "colab_type": "code",
        "outputId": "0d183755-606e-408c-cb1a-f9e59e462c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(470, 180)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqA0JoI00h4r",
        "colab_type": "code",
        "outputId": "5cbcf159-3d17-4a30-f78a-eca270722fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(470,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXAdR1WX00x1",
        "colab_type": "code",
        "outputId": "6ee75cb3-aa9f-42d3-ec5b-fcdde07e6e2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv1D,Flatten# Neural network\n",
        "input_shape = (180,1)\n",
        "num_files = x_train.shape[0]\n",
        "x_train = x_train.reshape(num_files,180,1)\n",
        "y_train = y_train.reshape(num_files)\n",
        "model0 = Sequential()\n",
        "model0.add(Conv1D(32, kernel_size=(3), input_shape=input_shape))\n",
        "model0.add(Conv1D(64, kernel_size=(3)))\n",
        "model0.add(Conv1D(128, kernel_size=(3)))\n",
        "model0.add(Conv1D(64, kernel_size=(3)))\n",
        "model0.add(Conv1D(32, kernel_size=(3)))\n",
        "model0.add(Flatten())\n",
        "model0.add(Dense(128, activation='relu'))\n",
        "model0.add(Dense(12, activation='relu'))\n",
        "model0.add(Dense(9, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsj9_-_RqMiU",
        "colab_type": "code",
        "outputId": "82e63c32-552f-421b-d9a7-90ab9506b80a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        }
      },
      "source": [
        "model0.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model0.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 178, 32)           128       \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 176, 64)           6208      \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 174, 128)          24704     \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 172, 64)           24640     \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 170, 32)           6176      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               696448    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 12)                1548      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 9)                 117       \n",
            "=================================================================\n",
            "Total params: 759,969\n",
            "Trainable params: 759,969\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yajP5ddbrd1j",
        "colab_type": "code",
        "outputId": "ea67af3d-73b5-4626-e6bb-7b9440e5641d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model0.fit(x_train, y_train, epochs=85,verbose=1, batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Epoch 1/85\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 1.7217 - accuracy: 0.3766\n",
            "Epoch 2/85\n",
            "470/470 [==============================] - 0s 634us/step - loss: 1.2297 - accuracy: 0.5106\n",
            "Epoch 3/85\n",
            "470/470 [==============================] - 0s 709us/step - loss: 1.0928 - accuracy: 0.5681\n",
            "Epoch 4/85\n",
            "470/470 [==============================] - 0s 624us/step - loss: 1.0853 - accuracy: 0.5681\n",
            "Epoch 5/85\n",
            "470/470 [==============================] - 0s 636us/step - loss: 0.8662 - accuracy: 0.6404\n",
            "Epoch 6/85\n",
            "470/470 [==============================] - 0s 636us/step - loss: 0.6715 - accuracy: 0.7106\n",
            "Epoch 7/85\n",
            "470/470 [==============================] - 0s 650us/step - loss: 0.8260 - accuracy: 0.7149\n",
            "Epoch 8/85\n",
            "470/470 [==============================] - 0s 690us/step - loss: 0.7911 - accuracy: 0.6894\n",
            "Epoch 9/85\n",
            "470/470 [==============================] - 0s 646us/step - loss: 0.5211 - accuracy: 0.7681\n",
            "Epoch 10/85\n",
            "470/470 [==============================] - 0s 666us/step - loss: 0.4360 - accuracy: 0.8298\n",
            "Epoch 11/85\n",
            "470/470 [==============================] - 0s 627us/step - loss: 0.4477 - accuracy: 0.8128\n",
            "Epoch 12/85\n",
            "470/470 [==============================] - 0s 622us/step - loss: 0.3642 - accuracy: 0.8617\n",
            "Epoch 13/85\n",
            "470/470 [==============================] - 0s 657us/step - loss: 0.2284 - accuracy: 0.9213\n",
            "Epoch 14/85\n",
            "470/470 [==============================] - 0s 628us/step - loss: 0.3010 - accuracy: 0.8872\n",
            "Epoch 15/85\n",
            "470/470 [==============================] - 0s 631us/step - loss: 0.2399 - accuracy: 0.9106\n",
            "Epoch 16/85\n",
            "470/470 [==============================] - 0s 655us/step - loss: 0.3225 - accuracy: 0.8872\n",
            "Epoch 17/85\n",
            "470/470 [==============================] - 0s 651us/step - loss: 0.3389 - accuracy: 0.8915\n",
            "Epoch 18/85\n",
            "470/470 [==============================] - 0s 638us/step - loss: 0.1614 - accuracy: 0.9468\n",
            "Epoch 19/85\n",
            "470/470 [==============================] - 0s 648us/step - loss: 0.0864 - accuracy: 0.9745\n",
            "Epoch 20/85\n",
            "470/470 [==============================] - 0s 657us/step - loss: 0.1063 - accuracy: 0.9660\n",
            "Epoch 21/85\n",
            "470/470 [==============================] - 0s 622us/step - loss: 0.2721 - accuracy: 0.9085\n",
            "Epoch 22/85\n",
            "470/470 [==============================] - 0s 614us/step - loss: 0.3242 - accuracy: 0.8872\n",
            "Epoch 23/85\n",
            "470/470 [==============================] - 0s 697us/step - loss: 0.1620 - accuracy: 0.9340\n",
            "Epoch 24/85\n",
            "470/470 [==============================] - 0s 618us/step - loss: 0.0711 - accuracy: 0.9745\n",
            "Epoch 25/85\n",
            "470/470 [==============================] - 0s 661us/step - loss: 0.1120 - accuracy: 0.9532\n",
            "Epoch 26/85\n",
            "470/470 [==============================] - 0s 668us/step - loss: 0.1650 - accuracy: 0.9362\n",
            "Epoch 27/85\n",
            "470/470 [==============================] - 0s 645us/step - loss: 0.9286 - accuracy: 0.8532\n",
            "Epoch 28/85\n",
            "470/470 [==============================] - 0s 618us/step - loss: 1.0662 - accuracy: 0.7723\n",
            "Epoch 29/85\n",
            "470/470 [==============================] - 0s 652us/step - loss: 0.2882 - accuracy: 0.9064\n",
            "Epoch 30/85\n",
            "470/470 [==============================] - 0s 645us/step - loss: 0.4609 - accuracy: 0.9064\n",
            "Epoch 31/85\n",
            "470/470 [==============================] - 0s 634us/step - loss: 0.1862 - accuracy: 0.9723\n",
            "Epoch 32/85\n",
            "470/470 [==============================] - 0s 646us/step - loss: 0.0918 - accuracy: 0.9681\n",
            "Epoch 33/85\n",
            "470/470 [==============================] - 0s 682us/step - loss: 0.0718 - accuracy: 0.9787\n",
            "Epoch 34/85\n",
            "470/470 [==============================] - 0s 623us/step - loss: 0.0998 - accuracy: 0.9766\n",
            "Epoch 35/85\n",
            "470/470 [==============================] - 0s 621us/step - loss: 0.0723 - accuracy: 0.9723\n",
            "Epoch 36/85\n",
            "470/470 [==============================] - 0s 662us/step - loss: 0.0640 - accuracy: 0.9745\n",
            "Epoch 37/85\n",
            "470/470 [==============================] - 0s 674us/step - loss: 0.0332 - accuracy: 0.9894\n",
            "Epoch 38/85\n",
            "470/470 [==============================] - 0s 606us/step - loss: 0.0256 - accuracy: 0.9936\n",
            "Epoch 39/85\n",
            "470/470 [==============================] - 0s 632us/step - loss: 0.0202 - accuracy: 0.9936\n",
            "Epoch 40/85\n",
            "470/470 [==============================] - 0s 641us/step - loss: 0.0599 - accuracy: 0.9851\n",
            "Epoch 41/85\n",
            "470/470 [==============================] - 0s 628us/step - loss: 0.0283 - accuracy: 0.9894\n",
            "Epoch 42/85\n",
            "470/470 [==============================] - 0s 654us/step - loss: 0.0251 - accuracy: 0.9915\n",
            "Epoch 43/85\n",
            "470/470 [==============================] - 0s 636us/step - loss: 0.0191 - accuracy: 0.9915\n",
            "Epoch 44/85\n",
            "470/470 [==============================] - 0s 633us/step - loss: 0.0186 - accuracy: 0.9915\n",
            "Epoch 45/85\n",
            "470/470 [==============================] - 0s 677us/step - loss: 0.0176 - accuracy: 0.9915\n",
            "Epoch 46/85\n",
            "470/470 [==============================] - 0s 641us/step - loss: 0.0159 - accuracy: 0.9915\n",
            "Epoch 47/85\n",
            "470/470 [==============================] - 0s 654us/step - loss: 0.0152 - accuracy: 0.9915\n",
            "Epoch 48/85\n",
            "470/470 [==============================] - 0s 644us/step - loss: 0.0339 - accuracy: 0.9872\n",
            "Epoch 49/85\n",
            "470/470 [==============================] - 0s 650us/step - loss: 0.3253 - accuracy: 0.9213\n",
            "Epoch 50/85\n",
            "470/470 [==============================] - 0s 654us/step - loss: 0.2939 - accuracy: 0.9043\n",
            "Epoch 51/85\n",
            "470/470 [==============================] - 0s 628us/step - loss: 0.0837 - accuracy: 0.9702\n",
            "Epoch 52/85\n",
            "470/470 [==============================] - 0s 642us/step - loss: 0.1194 - accuracy: 0.9660\n",
            "Epoch 53/85\n",
            "470/470 [==============================] - 0s 649us/step - loss: 0.0575 - accuracy: 0.9787\n",
            "Epoch 54/85\n",
            "470/470 [==============================] - 0s 629us/step - loss: 0.0451 - accuracy: 0.9872\n",
            "Epoch 55/85\n",
            "470/470 [==============================] - 0s 629us/step - loss: 0.0302 - accuracy: 0.9915\n",
            "Epoch 56/85\n",
            "470/470 [==============================] - 0s 643us/step - loss: 0.0230 - accuracy: 0.9936\n",
            "Epoch 57/85\n",
            "470/470 [==============================] - 0s 680us/step - loss: 0.0166 - accuracy: 0.9936\n",
            "Epoch 58/85\n",
            "470/470 [==============================] - 0s 628us/step - loss: 0.0130 - accuracy: 0.9936\n",
            "Epoch 59/85\n",
            "470/470 [==============================] - 0s 672us/step - loss: 0.0122 - accuracy: 0.9936\n",
            "Epoch 60/85\n",
            "470/470 [==============================] - 0s 689us/step - loss: 0.0116 - accuracy: 0.9936\n",
            "Epoch 61/85\n",
            "470/470 [==============================] - 0s 619us/step - loss: 0.0153 - accuracy: 0.9936\n",
            "Epoch 62/85\n",
            "470/470 [==============================] - 0s 670us/step - loss: 0.0265 - accuracy: 0.9915\n",
            "Epoch 63/85\n",
            "470/470 [==============================] - 0s 653us/step - loss: 1.1994 - accuracy: 0.8574\n",
            "Epoch 64/85\n",
            "470/470 [==============================] - 0s 622us/step - loss: 1.0660 - accuracy: 0.6553\n",
            "Epoch 65/85\n",
            "470/470 [==============================] - 0s 617us/step - loss: 0.3654 - accuracy: 0.8596\n",
            "Epoch 66/85\n",
            "470/470 [==============================] - 0s 639us/step - loss: 0.1938 - accuracy: 0.9255\n",
            "Epoch 67/85\n",
            "470/470 [==============================] - 0s 667us/step - loss: 0.1058 - accuracy: 0.9702\n",
            "Epoch 68/85\n",
            "470/470 [==============================] - 0s 638us/step - loss: 0.0702 - accuracy: 0.9766\n",
            "Epoch 69/85\n",
            "470/470 [==============================] - 0s 636us/step - loss: 0.2143 - accuracy: 0.9596\n",
            "Epoch 70/85\n",
            "470/470 [==============================] - 0s 661us/step - loss: 0.1376 - accuracy: 0.9681\n",
            "Epoch 71/85\n",
            "470/470 [==============================] - 0s 618us/step - loss: 0.1186 - accuracy: 0.9532\n",
            "Epoch 72/85\n",
            "470/470 [==============================] - 0s 647us/step - loss: 0.0430 - accuracy: 0.9830\n",
            "Epoch 73/85\n",
            "470/470 [==============================] - 0s 637us/step - loss: 0.0607 - accuracy: 0.9830\n",
            "Epoch 74/85\n",
            "470/470 [==============================] - 0s 659us/step - loss: 0.0754 - accuracy: 0.9723\n",
            "Epoch 75/85\n",
            "470/470 [==============================] - 0s 639us/step - loss: 0.0260 - accuracy: 0.9936\n",
            "Epoch 76/85\n",
            "470/470 [==============================] - 0s 652us/step - loss: 0.0134 - accuracy: 0.9936\n",
            "Epoch 77/85\n",
            "470/470 [==============================] - 0s 650us/step - loss: 0.0203 - accuracy: 0.9936\n",
            "Epoch 78/85\n",
            "470/470 [==============================] - 0s 649us/step - loss: 0.0126 - accuracy: 0.9936\n",
            "Epoch 79/85\n",
            "470/470 [==============================] - 0s 637us/step - loss: 0.0105 - accuracy: 0.9936\n",
            "Epoch 80/85\n",
            "470/470 [==============================] - 0s 654us/step - loss: 0.0103 - accuracy: 0.9936\n",
            "Epoch 81/85\n",
            "470/470 [==============================] - 0s 622us/step - loss: 0.0106 - accuracy: 0.9936\n",
            "Epoch 82/85\n",
            "470/470 [==============================] - 0s 639us/step - loss: 0.0097 - accuracy: 0.9936\n",
            "Epoch 83/85\n",
            "470/470 [==============================] - 0s 642us/step - loss: 0.0095 - accuracy: 0.9936\n",
            "Epoch 84/85\n",
            "470/470 [==============================] - 0s 649us/step - loss: 0.0100 - accuracy: 0.9936\n",
            "Epoch 85/85\n",
            "470/470 [==============================] - 0s 638us/step - loss: 0.0092 - accuracy: 0.9936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f3e623055c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w4CzoU-_LKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_testfiles = y_test.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZLfe24x_A92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = x_test.reshape(num_testfiles,180,1)\n",
        "y_test = y_test.reshape(num_testfiles)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxC48tvs-9zm",
        "colab_type": "code",
        "outputId": "bcbb8531-ff29-42b7-9940-0e4a366d42c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "model0.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "202/202 [==============================] - 0s 960us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.006552399975239, 0.6534653306007385]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOr-5TRjDGpw",
        "colab_type": "code",
        "outputId": "605598ec-5c9a-41c4-df84-ea7a665fe080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model0.predict(x_test[3].reshape(1,180,1)).argmax()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}